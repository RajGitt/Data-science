{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1812e8b9-1f1f-43b3-aaaa-6ee11f8005a8",
   "metadata": {},
   "source": [
    "<mark><b>Ensemble learning</mark> :- In ensemble learning, first we train different samples of data from dataset and then we train the different model on different samples, So we get different score for each model trained on different samples and then we aggregate the scores of each model as an final score. Ensemble learning givs us the unbiased score, as it is not taking the whole dataset to be trained as it will gives us the overfit model(where variance could be high) and in ensemble training we take subset of dataset as a sample(for ex. if we 100 samples then we take 70 samples in a subdataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d51e85f-3d47-484b-a12e-6879cd22745e",
   "metadata": {},
   "source": [
    "<mark>Bagging</b></mark> :- Bagging is an ensemble learning algorithm where we try to find the appropriate solution for aur provided dataset. Bagging is a classification model, where in bagging we take a different sub dataset samples from the dataset with the help of random sampling with replacement technique(In random sampling with replacement technique, while taking the different sub dataset samples(rows), if we have 100 samples in a dataset(rows) and we will take 70 samples(rows) in  a sub dataset then it is not neccesary that all the 70 samples would be different some of the samples in each dataset could be same as the random sampling with replacement technique takes the random data and can take that same data into teh sub dataset sample, as it does'nt take into accounting of which data it has choosen and because of random sampling with replacement some of the data would not be a part of the sub dataset samples, these are known as \"out of bag\" data and then we could use this data as test data(unseen data) while predicting the model) and then we build our different model on each different sub dataset sample and then we predict the score(r2(coefficient of determinant)) for each sub dataset samples and then we aggregate the score to get the 'final unbiased score'. \n",
    "\n",
    "<br>The sub dataset samples are known as 'bag' of data and this bagging technique is also known as bootstrap aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c239108-9416-4f29-867a-64d3c25f8f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "csv_file_src = r\"C:\\Users\\Hp\\Downloads\\diabetes.csv\"\n",
    "#if we are using a variable name to store teh csv file src/path then write the variable name without quotations \"\" and if we are importing the csv file directly then write the name of csv file with quoaions \"\".\n",
    "df = pd.read_csv(csv_file_src)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efeae7ac-6b69-4b75-b61e-c8cd9c956794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "#So, first we will check if there are any null/NAN(not a number) values in teh DataFrame by .info() function, and can check the statitics of the data as\n",
    "#well by df.describe()\n",
    "\n",
    "df.info()   #and we could have checked the null values by df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "870fa75f-5f1e-4549-95a0-5e635ace8e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnancies                 0\n",
       "Glucose                     0\n",
       "BloodPressure               0\n",
       "SkinThickness               0\n",
       "Insulin                     0\n",
       "BMI                         0\n",
       "DiabetesPedigreeFunction    0\n",
       "Age                         0\n",
       "Outcome                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#and we could have checked the null values by df.isnull().sum()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b287b5f8-0594-4616-a0bb-1de2c4adc2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we are getting the statistical data of the DataFrame df\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57d63ab6-6467-46ea-adc4-a3833d3f26ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pregnancy(max-17), so it seems to be an outlier but it is possible that a person have had pregnant for 17 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6f88c7f-8d68-4b62-a1c3-60460b5acfcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Outcome\n",
       "0    500\n",
       "1    268\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#So, here we are checking if our outcome column in dataset is balanced or not(as in how much data record we have of people who are diabetic and people who are not).\n",
    "df.Outcome.value_counts()   #df.Outcome.value_counts() will give us the count of our data record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12b8bd66-9127-4dba-9121-0f99e9bbeaaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so the data is slightly imbalanced as the data for diabetic people is much less(nearly 50% of non-diabetic people data).\n",
    "265/500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e09f89cb-c2f7-4248-9085-89f57e1feb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we are splitting the X and y to be trained.\n",
    "X = df.drop(\"Outcome\", axis=\"columns\")\n",
    "y = df.Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af0d7a17-7fe1-41bd-acb6-79676634462c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  \n",
       "0                     0.627   50  \n",
       "1                     0.351   31  \n",
       "2                     0.672   32  \n",
       "3                     0.167   21  \n",
       "4                     2.288   33  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e97b894d-0113-4c0f-950b-9c686d8af045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    1\n",
       "3    0\n",
       "4    1\n",
       "Name: Outcome, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bfd95642-22d3-4315-8166-ff6eb9eecd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.63994726,  0.84832379,  0.14964075,  0.90726993, -0.69289057,\n",
       "         0.20401277,  0.46849198,  1.4259954 ],\n",
       "       [-0.84488505, -1.12339636, -0.16054575,  0.53090156, -0.69289057,\n",
       "        -0.68442195, -0.36506078, -0.19067191],\n",
       "       [ 1.23388019,  1.94372388, -0.26394125, -1.28821221, -0.69289057,\n",
       "        -1.10325546,  0.60439732, -0.10558415]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here we will scale our data, so that the DataFrame could be scaled in -1 to 1 range, and the data could be presented well\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()   #instantiating the StandrdScaler object\n",
    "#sacling the X by fit_tranform() function, where fit takes the X(feature) as an input and tranform converts into the scaled data \n",
    "X_scaled = scaler.fit_transform(X)\n",
    "#and we don't need to scale the y as it's values are 0,1\n",
    "X_scaled[:3]  #this will return an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "40ec79a8-b67e-4762-98ef-0c8a5ca4e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we have sacled our feature data, no we'll train X and by train_test_split model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#because we have imbalanced dataset so, we'll use statify=y, so that each data of diabetic and non-diabetics are included while training our data\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ab65a68e-4ed7-4b2a-9fc4-80a6aac7fcd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614, 8)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ef14c866-13d9-45a3-bd86-4e19446bb6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5af7a10e-27c8-4ce9-9710-176391b7cf0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7efce208-7a6d-4f32-867e-71fcf1776d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.987012987012987"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "768/154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "be431842-36f6-46b1-9d5c-857e8d33b713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Outcome\n",
       "0    400\n",
       "1    214\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting the values of y_train\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "87087f40-e145-4540-b3c1-5d7b12fe97c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eb694a01-2416-4671-bc6e-83afe8061ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a68ac7b5-0cf2-4503-a3d9-8ba2b3b646a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe2d43-57de-4e3d-9389-ad1ca9d64974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a685f3ed-78c5-4757-a963-3aaeddaa0842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we'll use standalone decision tree classifier for bagging, as it is more effective but unstable classifier but when we use decision tree machine\n",
    "#learning algorithm with bagging it gets more effective as we take multiple samples and create sub datasets, so this lessens teh variance and lowers \n",
    "#the noise which comes from overfitting of data and then we'll see the benifits of using bagging(Ensemble learning).\n",
    "\"\"\"decision trees can be considered unstable classifiers. This instability is primarily due to their sensitivity to small changes in the training data.\n",
    "Here's a deeper look into why this is the case:\n",
    "\n",
    "1. Sensitivity to Data Variations\n",
    "Decision trees split the data based on certain conditions, creating a tree-like structure. Small variations in the training data, such as slight changes \n",
    "in the distribution or even a single outlier, can lead to different splits and, consequently, a different tree structure. This can cause significant\n",
    "variations in predictions, making the model unstable.\n",
    "\n",
    "2. High Variance\n",
    "Decision trees tend to have high variance, meaning that they can overfit the training data. Overfitting occurs when a model learns not only the underlying\n",
    "pattern but also the noise in the training data. As a result, the model performs well on the training data but poorly on unseen data. This overfitting \n",
    "contributes to the instability of decision trees.\n",
    "\n",
    "3. Mitigation Techniques\n",
    "Several techniques can help mitigate the instability of decision trees:\n",
    "\n",
    "Pruning: Reducing the size of the tree by removing branches that have little importance. This helps in reducing overfitting.\n",
    "Setting Minimum Samples per Leaf: By requiring a minimum number of samples in a leaf node, we can prevent the tree from growing too complex.\n",
    "Ensemble Methods: Techniques like bagging, random forests, and boosting can improve the stability and performance of decision trees. For instance, random\n",
    "forests build multiple decision trees on different subsets of the data and average their predictions, reducing variance and improving generalization.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b4f2d2bb-2e3c-4c7e-bb27-f8874944ee0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.71428571, 0.68181818, 0.67532468, 0.78431373, 0.7124183 ])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#now we'll use cross_val_score of K fold cross validation algorithm so that we can get better result.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "Scores = cross_val_score(DecisionTreeClassifier(),X,y,cv=5)   #cv will create 5 folds/split of dataset and will train all 5 folds model and give 5 scores for each\n",
    "Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5c749be4-512b-462c-93e3-e2fce038cb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7136321195144724"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6709d16e-86e2-4f3b-b1cc-95ec6494d7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7719869706840391"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we will use the bagging(ensemble learning) to check the score of the model\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#By default, if base_estimator=None, a decision tree classifier (DecisionTreeClassifier) is used. However, you can specify any other classifier or regressor \n",
    "#compatible with the scikit-learn API.\n",
    "bag_model = BaggingClassifier(\n",
    "    base_estimator = DecisionTreeClassifier(),   #here we ae using  Decision TreeClasssifier as the base model\n",
    "    n_estimators=100,   #here we are using 100 trees (as in 100 subdataset samples, so 100 models would be trained)\n",
    "    max_samples=0.8,     #max_samples=0.8 means that 80% of the total dataset would be used for creating the bag(sub dataset of data) for training the models\n",
    "    oob_score = True,    #oob_score=True means when random saampling with replacement, there would be some data which will be not taken by any of the samples/\n",
    "    #sub datasets created, those data's are known as \"out of bag\" data/samples hich ew can later use them as test data for predicting the model.\n",
    "    random_state=0\n",
    "    )\n",
    "\n",
    "#now we are fit the X_train and y_train\n",
    "bag_model.fit(X_train,y_train)\n",
    "bag_model.oob_score_     # bag_model.oob_score_ gives us the prediction on the out of bag(oob) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a07909fb-c1a3-4290-89ff-2cbad2843a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7597402597402597"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#.oob_score_ was the prediction computed on the oob(out of bag) which tells us that the r2 scorecould come near to this score\n",
    "#now we will check the score on X_test,y_test\n",
    "bag_model.score(X_test,y_test)    #this score was near"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ce402548-ba06-4936-aa52-1e98257323ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.75324675, 0.72727273, 0.74675325, 0.82352941, 0.73856209])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_model = BaggingClassifier(\n",
    "    base_estimator = DecisionTreeClassifier(), \n",
    "    #Instantiating the DecisionTreeClasssifier model #but it actually uses DecisionTreeClassifier as default base_estimator model\n",
    "    n_estimators=100,  #n_estimators gives 100 trees/model\n",
    "    max_samples=0.8,    #max_samples=0.8 will take 80% of the total dataset to be trained as different sample sub datasets\n",
    "    oob_score = True,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "#now we will sue the cross_val_score of K fold cross validation in bagging\n",
    "scores=cross_val_score(bag_model,X,y,cv=5)    #cv-=5 will create  folds or splits of the dataset\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "14aeebd6-c8f6-443e-95c1-bac053110d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7578728461081402"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96583f34-bf1f-4867-930f-87b587ca1f0f",
   "metadata": {},
   "source": [
    "<b>#so here we can see that when we used the DecisionTreeClassifier as a standalone model for bagging then it gave 72% acuraccy but now when we used DecisionTreeClassifier with bagging then it gave 75% accuracy, increased 3% of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "31c951ad-e57b-44a2-b695-e5053ed2c911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75324675, 0.71428571, 0.77272727, 0.83006536, 0.77777778])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here we will see the accuracy of the RandomForestClassifier, RandomForrestClassifier do the bagging on dataset(rows) and features smpling(feature subsetting) on columns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#we'll use cross_val_score\n",
    "Score=cross_val_score(RandomForestClassifier(n_estimators=50),X,y,cv=5)  #n_estimators=50 means that it will create 50 trees/nodes/models(RandomForestClassifier)\n",
    "#cv=5 means five folds/splits of the dataset(K fold cross validation)\n",
    "Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cec282fb-1b26-45c2-9b43-eacef551fb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7696205755029285"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f64674-012c-4263-b845-d0f0bf122348",
   "metadata": {},
   "source": [
    "<b>#here as we can see the score for RandomForestClassifier is more than the DesicionTreeClasifier and BaggingClassifier, as the RandomForestClassifier is very robust machine learning algorithm and RandomForestClassifier uses BaggingClassifier model internally. The RandomForestClassifier uses the bagging and features for sampling\n",
    "\n",
    "\"\"\"1. Bagging (Bootstrap Aggregating)\n",
    "Bagging involves creating multiple subsets of the dataset by randomly sampling the data points (rows) with replacement. Each subset is used to train a separate model (like a decision tree).\n",
    "\n",
    "Example:\n",
    "Let's say we create three subsets for bagging:\n",
    "\n",
    "Subset 1:\n",
    "\n",
    "Row 2: (8, 183, 64, 0, 0, 23.3, 0.672, 32)\n",
    "Row 4: (0, 137, 40, 35, 168, 43.1, 2.288, 33)\n",
    "Row 1: (1, 85, 66, 29, 0, 26.6, 0.351, 31)\n",
    "Row 1: (1, 85, 66, 29, 0, 26.6, 0.351, 31)\n",
    "Row 3: (1, 89, 66, 23, 94, 28.1, 0.167, 21)\n",
    "Subset 2:\n",
    "\n",
    "Row 0: (6, 148, 72, 35, 0, 33.6, 0.627, 50)\n",
    "Row 4: (0, 137, 40, 35, 168, 43.1, 2.288, 33)\n",
    "Row 3: (1, 89, 66, 23, 94, 28.1, 0.167, 21)\n",
    "Row 0: (6, 148, 72, 35, 0, 33.6, 0.627, 50)\n",
    "Row 2: (8, 183, 64, 0, 0, 23.3, 0.672, 32)\n",
    "Subset 3:\n",
    "\n",
    "Row 4: (0, 137, 40, 35, 168, 43.1, 2.288, 33)\n",
    "Row 3: (1, 89, 66, 23, 94, 28.1, 0.167, 21)\n",
    "Row 2: (8, 183, 64, 0, 0, 23.3, 0.672, 32)\n",
    "Row 2: (8, 183, 64, 0, 0, 23.3, 0.672, 32)\n",
    "Row 1: (1, 85, 66, 29, 0, 26.6, 0.351, 31)\n",
    "Each subset may contain duplicates and omit some original data points. Each subset is used to train a separate model, and the results are aggregated (e.g., by averaging predictions or taking a majority vote) to make the final prediction. This aggregation helps reduce the model's variance.\n",
    "\n",
    "2. Feature Sampling (Feature Subsetting)\n",
    "Feature sampling involves randomly selecting a subset of features (columns) to use at each decision point in each tree. This is done to introduce variability among the trees in the ensemble, further reducing correlation and improving robustness.\n",
    "\n",
    "Example:\n",
    "For each decision tree in the ensemble:\n",
    "\n",
    "At the first split, one tree might consider only Glucose, BloodPressure, and BMI to decide the split, ignoring other features.\n",
    "Another tree might use Pregnancies, Insulin, and Age.\n",
    "The choice of features to consider at each split is random, making each tree see the data differently, even if the training data points are the same.\n",
    "Combined Use in Random Forests\n",
    "In a Random Forest, both bagging and feature sampling are employed:\n",
    "\n",
    "Bagging: Each tree is trained on a bootstrapped sample of the data.\n",
    "Feature Sampling: Each split in a tree considers only a random subset of features.\n",
    "This dual randomness (in data and feature selection) helps Random Forests to be powerful, reducing overfitting and enhancing the model's generalization to new data. The final prediction is made by aggregating the predictions from all the individual trees, such as by majority vote for classification or averaging for regression.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe34d7b-0fa1-470a-8e3c-ccc067ec39cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
